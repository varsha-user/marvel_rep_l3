# marvel_rep_l3

## 1. DECISION TREES

Learnt about Decision Tress and how it works. Learnt how a tree consists of root node, parent node, decision node, leaf node, etc. Also learnt about pruning and splitting. Also learnt about ID3 algorithm and how it works. 
![WhatsApp Image 2025-01-03 at 13 50 32_3c86e7b6](https://github.com/user-attachments/assets/3e7e10d9-6751-43fa-a389-97232501f68e)
![WhatsApp Image 2025-01-03 at 13 50 56_6ddbc174](https://github.com/user-attachments/assets/58438e3f-45a9-4a94-8fde-51d257c4c8cf)
![WhatsApp Image 2025-01-03 at 13 51 13_8148b8e7](https://github.com/user-attachments/assets/60252e99-b28e-45d0-81fd-b3e31a22440f)

[Implementation of Decision tress on the Plays Tennis dataset](https://colab.research.google.com/drive/1o7HnzMapUYYitPdF0pvUFKtXqoDbDW_T?usp=sharing)

## 2. NAIVE BAYES

Learnt about what Naive Bayes is, how it works. Got a small recap of Bayes Theorem, from third sem, unit 5 maths ðŸ˜­
Then, implemented it from scratch on a diabetes dataset. 
![WhatsApp Image 2025-03-02 at 22 55 02_dcf39bc0](https://github.com/user-attachments/assets/31f04c14-fb2b-443c-b798-7c9b779ad694)
![WhatsApp Image 2025-03-02 at 22 55 15_a6ee6c27](https://github.com/user-attachments/assets/52a9bdd3-0d06-44f7-8742-454c6aaf14c2)
[Implementation of Naive Bayes on the Diabetes dataset](https://colab.research.google.com/drive/1VHYBERsAy4Pb13ZZvaal8t0Pye8-V1P_?usp=sharing)

## 3. AND 4. ENSEMBLE TECHNIQUES AND RANDOM FORESTS, GRADIENT BOOST MACHINE(GBM) AND XGBOOST

Learnt about different ensemble techniques and understood how they help in improving model accuracies and help in better prediction and classification. 
I learnt about:
* Bagging
* Boosting
* Stacking
  And I implemented it on the Titanic dataset. Under the implementation of boosting itself I have covered the implementation of both GBM and XGboost for the same Titanic dataset.
![image](https://github.com/user-attachments/assets/338b89ba-6090-44f1-9b71-121c23f95bf2)
![Screenshot 2025-03-23 195119](https://github.com/user-attachments/assets/d25b05b3-7143-4376-8a9d-ddc99c421b72)
[Implementation of Ensemble methods on the Titanic Dataset](https://colab.research.google.com/drive/1Q2qzXytnvE8rWrXQfxMWgixSbojk9jJ5?usp=sharing)
And Random forests,
[Implementation of Random Forests on the Iris Dataset](https://colab.research.google.com/drive/17G6KJZJ1i7NudinlD5Wfei9tgWkjSLc2?usp=sharing)

## 6. K MEANS CLUSTERING

K-means clustering is an unsupervised learning ML algorithm, which forms clusters by initialising some random data points as centroids and assigning other data points to ach of these clusters. The 'k' here is the number of clusters. Once the clusters are formed, their mean value is assigned as the new centroid and this process is iterated till there is no more change in the new clusters are formed. Here, the clustering happens by finding out the Euclidian distance of the point from the centroid. It is computationally efficient and is ideal for use cases like: *1) Fraud detection *2) Shopping store customer grouping and optimisation *3) Document classification
I also briefly went through alternative options for unsupervised learning ML algos and ways to improve K-means clustering.

I used the MNIST dataset available in Google Colab and imported it for implementing K-means clustering on that MNIST Dataset. 
[Implementing K-means clustering on MNIST Dataset](https://colab.research.google.com/drive/1woO4XHX0A_tkKc1rFbX80vJOVsxYYNzc?usp=sharing)


